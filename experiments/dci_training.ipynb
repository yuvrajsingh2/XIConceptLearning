{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from BaseVAEs.models.disent.model.ae import EncoderConv64, DecoderConv64, AutoEncoder\n",
    "from BaseVAEs.models.disent.frameworks.vae.weaklysupervised import AdaVae, AdaCatVae\n",
    "from BaseVAEs.models.disent.frameworks.vae.unsupervised import BetaVae\n",
    "from BaseVAEs.models.disent.metrics import metric_dci, metric_mig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import sys\n",
    "import os\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from rtpt.rtpt import RTPT\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import BaseVAEs.utils_disent as utils\n",
    "import BaseVAEs.data as data\n",
    "from BaseVAEs.args import parse_args_as_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device name: cuda:0\n"
     ]
    }
   ],
   "source": [
    "sys_argv = [\n",
    "    \"--save-step\", \"20\",\n",
    "    \"--print-step\", \"1\",\n",
    "    \"--learning-rate\", \"0.0001\",\n",
    "    \"--batch-size\", \"128\",\n",
    "    \"--epochs\", \"200\",\n",
    "    \"--exp-name\", \"unsup-betavae-0-ecr\",\n",
    "    \"--n-groups\", \"4\",\n",
    "    \"--n-protos\", \"6\",\n",
    "    \"--seed\", \"0\",\n",
    "    \"--dataset\", \"ecr\",\n",
    "    \"--initials\", \"YS\",\n",
    "    \"--lr-scheduler-warmup-steps\", \"1000\",\n",
    "    \"--data-dir\", \"Data\",\n",
    "    \"--results-dir\", \"experiments/BaseVAEs/runs/\",\n",
    "    \"--n-workers\", \"0\"\n",
    "]\n",
    "\n",
    "config = parse_args_as_dict(sys_argv)\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train(model, data_loader, log_samples, optimizer, scheduler, writer, config):\n",
    "    # Initialize the RTPT (Real-Time Progress Tracking) object for monitoring training progress\n",
    "    rtpt = RTPT(name_initials=config['initials'], experiment_name='XIC_PrototypeDL', max_iterations=config['epochs'])\n",
    "    rtpt.start()\n",
    "\n",
    "    # Initialize warmup steps for learning rate scheduling\n",
    "    warmup_steps = 0\n",
    "\n",
    "    # Main training loop over epochs\n",
    "    for e in range(config['epochs']):\n",
    "        # Calculate the maximum number of iterations for the progress tracker\n",
    "        max_iter = len(data_loader)\n",
    "        # Record the start time of the epoch for performance tracking\n",
    "        start = time.time()\n",
    "        # Initialize a dictionary to hold various loss components\n",
    "        loss_dict = {'z_recon_loss': 0, 'loss': 0, 'kld': 0, 'elbo': 0}\n",
    "\n",
    "        # Loop over batches of data\n",
    "        for i, batch in enumerate(data_loader):\n",
    "            # Manually adjust learning rate for warmup\n",
    "            if warmup_steps < config['lr_scheduler_warmup_steps']:\n",
    "                learning_rate = config['learning_rate'] * (warmup_steps + 1) / config['lr_scheduler_warmup_steps']\n",
    "                optimizer.param_groups[0]['lr'] = learning_rate\n",
    "            warmup_steps += 1\n",
    "\n",
    "            # Unpack and prepare the data\n",
    "            imgs, labels_one_hot, labels_id, shared_labels = batch\n",
    "            imgs0 = imgs[0].to(config['device'])\n",
    "            imgs1 = imgs[1].to(config['device'])\n",
    "            imgs = torch.cat((imgs0, imgs1), dim=0)\n",
    "\n",
    "            # Prepare the batch for the model\n",
    "            batch = {'x': (imgs,), 'x_targ': (imgs,)}\n",
    "            # Compute the training loss\n",
    "            batch_loss_dict = model.compute_training_loss(batch, batch_idx=i)\n",
    "\n",
    "            # Extract various loss components\n",
    "            loss, recon_loss, kl_reg_loss, kl_loss, elbo = \\\n",
    "                batch_loss_dict['train_loss'], batch_loss_dict['recon_loss'], batch_loss_dict['kl_reg_loss'], \\\n",
    "                batch_loss_dict['kl_loss'], batch_loss_dict['elbo']\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Learning rate scheduling\n",
    "            if config['lr_scheduler'] and e > config['lr_scheduler_warmup_steps']:\n",
    "                scheduler.step()\n",
    "\n",
    "            # Accumulate loss values\n",
    "            loss_dict['z_recon_loss'] += recon_loss.item()\n",
    "            loss_dict['kld'] += kl_reg_loss.item()\n",
    "            loss_dict['loss'] += loss.item()\n",
    "            loss_dict['elbo'] += elbo.item()\n",
    "\n",
    "        # Calculate average loss over the epoch\n",
    "        for key in loss_dict.keys():\n",
    "            loss_dict[key] /= len(data_loader)\n",
    "\n",
    "        # Update progress tracker\n",
    "        rtpt.step(subtitle=f'loss={loss_dict[\"loss\"]:2.2f}')\n",
    "\n",
    "        # Log and print training progress\n",
    "        if (e + 1) % config['display_step'] == 0 or e == config['epochs'] - 1:\n",
    "            cur_lr = optimizer.param_groups[0][\"lr\"]\n",
    "            writer.add_scalar(\"lr\", cur_lr, global_step=e)\n",
    "            for key in loss_dict.keys():\n",
    "                writer.add_scalar(f'train/{key}', loss_dict[key], global_step=e)\n",
    "\n",
    "        if (e + 1) % config['print_step'] == 0 or e == config['epochs'] - 1:\n",
    "            print(f'epoch {e} - loss {loss.item():2.4f} - time/epoch {(time.time() - start):2.2f}')\n",
    "            loss_summary = ' '.join([f'{key} {value:2.4f}' for key, value in loss_dict.items()])\n",
    "            print(loss_summary)\n",
    "\n",
    "        # Save model and log samples\n",
    "        if (e + 1) % config['save_step'] == 0 or e == config['epochs'] - 1 or e == 0:\n",
    "            state = {\n",
    "                'model': model.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'ep': e,\n",
    "                'config': config\n",
    "            }\n",
    "            torch.save(state, os.path.join(config['model_dir'], '%05d.pth' % (e)))\n",
    "            utils.plot_examples(log_samples, model, writer, config, step=e)\n",
    "            print(f'SAVED - epoch {e} - imgs @ {config[\"img_dir\"]} - model @ {config[\"model_dir\"]}')\n",
    "            \n",
    "    \n",
    "\n",
    "\n",
    "def main(config):\n",
    "\n",
    "    # get train data\n",
    "    _data_loader = data.get_dataloader(config)\n",
    "\n",
    "    # get test set samples\n",
    "    test_set = data.get_test_set(_data_loader, config)\n",
    "\n",
    "    # create tb writer\n",
    "    writer = SummaryWriter(log_dir=config['results_dir'])\n",
    "\n",
    "    # model setup\n",
    "    _model = BetaVae(make_optimizer_fn=lambda params: Adam(params, lr=1e-3),\n",
    "                 make_model_fn=lambda: AutoEncoder(\n",
    "                     encoder=EncoderConv64(x_shape=(3, 64, 64), z_size=config['n_groups'], z_multiplier=2),\n",
    "                     decoder=DecoderConv64(x_shape=(3, 64, 64), z_size=config['n_groups']),\n",
    "                 ),\n",
    "                 cfg=BetaVae.cfg(beta=4))\n",
    "\n",
    "    _model = _model.to(config['device'])\n",
    "\n",
    "    # optimizer setup\n",
    "    optimizer = torch.optim.Adam(_model.parameters(), lr=config['learning_rate'])\n",
    "\n",
    "    # learning rate scheduler\n",
    "    scheduler = None\n",
    "    if config['lr_scheduler']:\n",
    "        # TODO: try LambdaLR\n",
    "        num_steps = len(_data_loader) * config['epochs']\n",
    "        num_steps += config['lr_scheduler_warmup_steps']\n",
    "        scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_steps, eta_min=2e-5)\n",
    "\n",
    "    # start training\n",
    "    train(_model, _data_loader, test_set, optimizer, scheduler, writer, config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting dataloader for ecr\n",
      "Loading data...\n",
      "Dataset: ecr\n",
      "root path: Data/ECR\\ECR\n",
      "root path: c:\\Users\\yuviu\\Desktop\\Uni Work\\Thesis\\XIConceptLearning\\experiments\\Data\\ECR\\ECR\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Path Data/ECR does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\yuviu\\Desktop\\Uni Work\\Thesis\\XIConceptLearning\\experiments\\load_icsn.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/yuviu/Desktop/Uni%20Work/Thesis/XIConceptLearning/experiments/load_icsn.ipynb#X13sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m main(config\u001b[39m=\u001b[39;49mconfig)\n",
      "\u001b[1;32mc:\\Users\\yuviu\\Desktop\\Uni Work\\Thesis\\XIConceptLearning\\experiments\\load_icsn.ipynb Cell 6\u001b[0m line \u001b[0;36m9\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/yuviu/Desktop/Uni%20Work/Thesis/XIConceptLearning/experiments/load_icsn.ipynb#X13sZmlsZQ%3D%3D?line=91'>92</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmain\u001b[39m(config):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/yuviu/Desktop/Uni%20Work/Thesis/XIConceptLearning/experiments/load_icsn.ipynb#X13sZmlsZQ%3D%3D?line=92'>93</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/yuviu/Desktop/Uni%20Work/Thesis/XIConceptLearning/experiments/load_icsn.ipynb#X13sZmlsZQ%3D%3D?line=93'>94</a>\u001b[0m     \u001b[39m# get train data\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/yuviu/Desktop/Uni%20Work/Thesis/XIConceptLearning/experiments/load_icsn.ipynb#X13sZmlsZQ%3D%3D?line=94'>95</a>\u001b[0m     _data_loader \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39;49mget_dataloader(config)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/yuviu/Desktop/Uni%20Work/Thesis/XIConceptLearning/experiments/load_icsn.ipynb#X13sZmlsZQ%3D%3D?line=96'>97</a>\u001b[0m     \u001b[39m# get test set samples\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/yuviu/Desktop/Uni%20Work/Thesis/XIConceptLearning/experiments/load_icsn.ipynb#X13sZmlsZQ%3D%3D?line=97'>98</a>\u001b[0m     test_set \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mget_test_set(_data_loader, config)\n",
      "File \u001b[1;32mc:\\Users\\yuviu\\Desktop\\Uni Work\\Thesis\\XIConceptLearning\\experiments\\BaseVAEs\\data.py:11\u001b[0m, in \u001b[0;36mget_dataloader\u001b[1;34m(config)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_dataloader\u001b[39m(config):\n\u001b[0;32m     10\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mGetting dataloader for \u001b[39m\u001b[39m{\u001b[39;00mconfig[\u001b[39m'\u001b[39m\u001b[39mdataset\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 11\u001b[0m     dataset \u001b[39m=\u001b[39m load_data(config)\n\u001b[0;32m     12\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m Config num_workers: \u001b[39m\u001b[39m{\u001b[39;00mconfig[\u001b[39m'\u001b[39m\u001b[39mn_workers\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     13\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataLoader(dataset, batch_size\u001b[39m=\u001b[39mconfig[\u001b[39m'\u001b[39m\u001b[39mbatch_size\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m     14\u001b[0m                                        shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, num_workers\u001b[39m=\u001b[39mconfig[\u001b[39m'\u001b[39m\u001b[39mn_workers\u001b[39m\u001b[39m'\u001b[39m], pin_memory\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\yuviu\\Desktop\\Uni Work\\Thesis\\XIConceptLearning\\experiments\\BaseVAEs\\data.py:21\u001b[0m, in \u001b[0;36mload_data\u001b[1;34m(config)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[39m# dataloader setup\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[39mif\u001b[39;00m config[\u001b[39m'\u001b[39m\u001b[39mdataset\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mecr\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m---> 21\u001b[0m     dataset \u001b[39m=\u001b[39m ECR_PairswithTest(config[\u001b[39m'\u001b[39;49m\u001b[39mdata_dir\u001b[39;49m\u001b[39m'\u001b[39;49m], attrs\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m     22\u001b[0m     config[\u001b[39m'\u001b[39m\u001b[39mimg_shape\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m (\u001b[39m3\u001b[39m, \u001b[39m64\u001b[39m, \u001b[39m64\u001b[39m)\n\u001b[0;32m     23\u001b[0m     \u001b[39mreturn\u001b[39;00m dataset\n",
      "File \u001b[1;32mc:\\Users\\yuviu\\Desktop\\Uni Work\\Thesis\\XIConceptLearning\\experiments\\BaseVAEs\\data.py:78\u001b[0m, in \u001b[0;36mECR_PairswithTest.__init__\u001b[1;34m(self, root, attrs, mode, single)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[39m# print as absolute path\u001b[39;00m\n\u001b[0;32m     77\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mroot path: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mabspath(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mroot)))\n\u001b[1;32m---> 78\u001b[0m \u001b[39massert\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mroot), \u001b[39m'\u001b[39m\u001b[39mPath \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m does not exist\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(root)\n\u001b[0;32m     80\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mLoading \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39msep\u001b[39m.\u001b[39mjoin([\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mroot, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtrain_ecr\u001b[39m\u001b[39m{\u001b[39;00mattrs\u001b[39m}\u001b[39;00m\u001b[39m_pairs.npy\u001b[39m\u001b[39m\"\u001b[39m]))\n\u001b[0;32m     82\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_data_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39msep\u001b[39m.\u001b[39mjoin([\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mroot, mode, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mmode\u001b[39m}\u001b[39;00m\u001b[39m_ecr\u001b[39m\u001b[39m{\u001b[39;00mattrs\u001b[39m}\u001b[39;00m\u001b[39m_pairs.npy\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "\u001b[1;31mAssertionError\u001b[0m: Path Data/ECR does not exist"
     ]
    }
   ],
   "source": [
    "main(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pretrained(model, ckpt):\n",
    "    model.load_state_dict(ckpt['model'])\n",
    "    model.proto_dict = ckpt['model_misc']['prototypes']\n",
    "    model.softmax_temp = ckpt['model_misc']['softmax_temp']\n",
    "    return model\n",
    "\n",
    "from ProtoLearning.models.icsn import iCSN\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yuviu\\Desktop\\Uni Work\\Thesis\\XIConceptLearning\n",
      "c:\\Users\\yuviu\\Desktop\\Uni Work\\Thesis\\XIConceptLearning\\experiments\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "\n",
    "# change to ../\n",
    "os.chdir('experiments')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "proto-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
